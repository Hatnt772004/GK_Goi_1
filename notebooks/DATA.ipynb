{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849797e2",
   "metadata": {},
   "source": [
    "## 1. M√£ ch·ª©ng kho√°n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d142867",
   "metadata": {},
   "source": [
    "1.1 Thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠ - C∆° ch·∫ø t·∫£i: T·ª± ƒë·ªông x·ª≠ l√Ω ph√¢n trang. \n",
    "\n",
    "1.2. Chu·∫©n h√≥a d·ªØ li·ªáu: ƒê·ªïi t√™n c·ªôt; X·ª≠ l√Ω ƒë·ªãnh d·∫°ng\n",
    "\n",
    "1.3. L∆∞u tr·ªØ: l∆∞u th√†nh t·ª´ng file CSV ri√™ng bi·ªát cho m·ªói m√£ c·ªï phi·∫øu (v√≠ d·ª•: ACB.csv, VCB.csv). Th∆∞ m·ª•c l∆∞u tr·ªØ l√† data_ohlcv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68c3b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# --- 1. C·∫§U H√åNH CHUNG ---\n",
    "STOCK_LIST = [\n",
    "    'ACB', 'BID', 'CTG', 'EIB', 'HDB', 'LPB', 'MBB', 'MSB', \n",
    "    'NAB', 'OCB', 'SHB', 'SSB', 'STB', 'TCB', 'TPB', 'VCB', \n",
    "    'VIB', 'VPB'\n",
    "]\n",
    "\n",
    "# --- THAY ƒê·ªîI 1: L·∫•y t·ª´ 2019 ---\n",
    "START_DATE_CAFEF = '01/01/2019'\n",
    "END_DATE_CAFEF = '01/01/2025' # Gi·ªØ nguy√™n ƒë·ªÉ bao g·ªìm h·∫øt 2024\n",
    "\n",
    "DATA_FOLDER = 'data_ohlcv' # ƒê·ªïi t√™n th∆∞ m·ª•c ƒë·ªÉ tr√°nh ghi ƒë√® file c≈©\n",
    "CAFEF_API_URL = 'https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx'\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://cafef.vn/'\n",
    "}\n",
    "\n",
    "# --- 2. H√ÄM T·∫¢I D·ªÆ LI·ªÜU (ƒê√£ n√¢ng c·∫•p ƒë·ªÉ l·∫•y OHLCV-A) ---\n",
    "def get_stock_data_from_cafef(symbol, start_date, end_date):\n",
    "    \"\"\"\n",
    "    T·∫£i d·ªØ li·ªáu OHLCV-A cho 1 m√£ c·ªï phi·∫øu t·ª´ CafeF, x·ª≠ l√Ω ph√¢n trang.\n",
    "    \"\"\"\n",
    "    print(f\"    ƒêang t·∫£i {symbol} t·ª´ CafeF...\")\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    session = requests.Session()\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'Symbol': symbol,\n",
    "            'StartDate': start_date,\n",
    "            'EndDate': end_date,\n",
    "            'PageIndex': page,\n",
    "            'PageSize': 50\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = session.get(CAFEF_API_URL, params=params, headers=REQUEST_HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            try:\n",
    "                response_json = response.json()\n",
    "            except requests.exceptions.JSONDecodeError:\n",
    "                print(f\"    L·ªñI JSON: CafeF tr·∫£ v·ªÅ text l·ªói cho {symbol} trang {page}.\")\n",
    "                break\n",
    "\n",
    "            if 'Data' in response_json and 'Data' in response_json['Data']:\n",
    "                actual_data_list = response_json['Data']['Data']\n",
    "            else:\n",
    "                print(f\"    L·ªñI C·∫§U TR√öC: JSON tr·∫£ v·ªÅ cho {symbol} kh√¥ng c√≥ key 'Data' b√™n trong.\")\n",
    "                break\n",
    "\n",
    "            if not actual_data_list:\n",
    "                print(f\"    -> {symbol}: Trang {page} r·ªóng, k·∫øt th√∫c t·∫£i.\")\n",
    "                break\n",
    "                \n",
    "            all_data.extend(actual_data_list)\n",
    "            page += 1\n",
    "            time.sleep(0.2) # Gi·ªØ nguy√™n sleep\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"    L·ªñI M·∫†NG/HTTP khi t·∫£i {symbol} (trang {page}): {e}\")\n",
    "            break\n",
    "            \n",
    "    if not all_data:\n",
    "        print(f\"    -> Kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu cho {symbol}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- THAY ƒê·ªîI 2: X·ª≠ l√Ω DataFrame ƒë·ªÉ l·∫•y OHLCV-A ---\n",
    "    try:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # 2a. C√°c c·ªôt c·∫ßn thi·∫øt t·ª´ CafeF\n",
    "        COLUMNS_TO_KEEP = [\n",
    "            'Ngay', \n",
    "            'GiaMoCua', \n",
    "            'GiaCaoNhat', \n",
    "            'GiaThapNhat', \n",
    "            'GiaDongCua', \n",
    "            'GiaDieuChinh', \n",
    "            'KhoiLuongKhopLenh' # Th∆∞·ªùng l√† 'KhoiLuongKhopLenh' ho·∫∑c 'KLGDKL'\n",
    "        ]\n",
    "        \n",
    "        # 2b. ƒê·ªïi t√™n sang chu·∫©n ti·∫øng Anh (quan tr·ªçng cho pandas-ta)\n",
    "        RENAME_MAP = {\n",
    "            'Ngay': 'date_str', # T·∫°m th·ªùi\n",
    "            'GiaMoCua': 'open',\n",
    "            'GiaCaoNhat': 'high',\n",
    "            'GiaThapNhat': 'low',\n",
    "            'GiaDongCua': 'close',\n",
    "            'GiaDieuChinh': 'adjusted_close',\n",
    "            'KhoiLuongKhopLenh': 'volume'\n",
    "        }\n",
    "        \n",
    "        # 2c. Ki·ªÉm tra xem c√≥ ƒë·ªß c√°c c·ªôt kh√¥ng\n",
    "        if not all(col in df.columns for col in COLUMNS_TO_KEEP):\n",
    "            print(f\"    L·ªñI C·ªòT: D·ªØ li·ªáu cho {symbol} thi·∫øu c√°c c·ªôt OHLCV.\")\n",
    "            print(f\"    C√°c c·ªôt c√≥: {list(df.columns)}\")\n",
    "            # Th·ª≠ ki·ªÉm tra t√™n c·ªôt kh·ªëi l∆∞·ª£ng thay th·∫ø\n",
    "            if 'KLGDKL' in df.columns and 'KhoiLuongKhopLenh' not in df.columns:\n",
    "                print(\"    T√¨m th·∫•y 'KLGDKL', ƒëang th·ª≠ l·∫°i...\")\n",
    "                COLUMNS_TO_KEEP.remove('KhoiLuongKhopLenh')\n",
    "                COLUMNS_TO_KEEP.append('KLGDKL')\n",
    "                RENAME_MAP['KLGDKL'] = 'volume'\n",
    "            else:\n",
    "                return pd.DataFrame() # B·ªè qua n·∫øu v·∫´n thi·∫øu\n",
    "        \n",
    "        # 2d. Ch·ªçn v√† ƒë·ªïi t√™n c·ªôt\n",
    "        df = df[COLUMNS_TO_KEEP]\n",
    "        df = df.rename(columns=RENAME_MAP)\n",
    "        \n",
    "        # 2e. Chuy·ªÉn ƒë·ªïi ng√†y v√† ƒë·∫∑t l√†m index\n",
    "        df['date'] = pd.to_datetime(df['date_str'], format='%d/%m/%Y')\n",
    "        df = df.set_index('date')\n",
    "        \n",
    "        # 2f. ƒê·∫£m b·∫£o ƒë√∫ng ki·ªÉu d·ªØ li·ªáu (R·∫§T QUAN TR·ªåNG)\n",
    "        numeric_cols = ['open', 'high', 'low', 'close', 'adjusted_close', 'volume']\n",
    "        # X√≥a d·∫•u ph·∫©y (,) n·∫øu c√≥, v√≠ d·ª•: 1,200\n",
    "        for col in numeric_cols:\n",
    "            if df[col].dtype == 'object':\n",
    "                 df[col] = df[col].astype(str).str.replace(',', '', regex=False)\n",
    "                     \n",
    "        df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # 2g. S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt chu·∫©n v√† tr·∫£ v·ªÅ\n",
    "        df = df[['open', 'high', 'low', 'close', 'adjusted_close', 'volume']]\n",
    "        \n",
    "        print(f\"    -> T·∫£i v√† x·ª≠ l√Ω OHLCV-A cho {symbol} th√†nh c√¥ng ({len(df)} d√≤ng).\")\n",
    "        return df\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"    L·ªñI KEY_ERROR: {e}. C·∫•u tr√∫c JSON cho {symbol} kh√¥ng nh∆∞ mong ƒë·ª£i.\")\n",
    "        if 'df' in locals():\n",
    "            print(f\"    C√°c c·ªôt c√≥: {list(df.columns)}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"    L·ªñI X·ª¨ L√ù PANDAS cho {symbol}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- 4. CH·∫†Y CH∆Ø∆†NG TR√åNH CH√çNH ---\n",
    "print(\"B·∫ÆT ƒê·∫¶U QU√Å TR√åNH T·∫¢I V√Ä L∆ØU D·ªÆ LI·ªÜU C·ªî PHI·∫æU (OHLCV-A)\")\n",
    "\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "print(f\"D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c: '{DATA_FOLDER}/'\")\n",
    "\n",
    "print(\"\\nB·∫Øt ƒë·∫ßu t·∫£i d·ªØ li·ªáu c·ªï phi·∫øu t·ª´ CafeF...\")\n",
    "\n",
    "successful_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for stock in STOCK_LIST:\n",
    "    stock_df = get_stock_data_from_cafef(stock, START_DATE_CAFEF, END_DATE_CAFEF)\n",
    "    \n",
    "    if not stock_df.empty:\n",
    "        try:\n",
    "            file_path = os.path.join(DATA_FOLDER, f\"{stock}.csv\")\n",
    "            stock_df.to_csv(file_path, index=True) # L∆∞u file CSV v·ªõi ƒë·∫ßy ƒë·ªß c·ªôt\n",
    "            \n",
    "            print(f\"    ‚úÖ ƒê√£ l∆∞u {stock} v√†o {file_path}\")\n",
    "            successful_count += 1\n",
    "            \n",
    "        except (OSError, PermissionError) as e:\n",
    "            print(f\"    ‚ùå L·ªñI KHI L∆ØU FILE {stock}: {e}\")\n",
    "            failed_count += 1\n",
    "    else:\n",
    "        print(f\"    ‚ùå B·ªè qua {stock} do kh√¥ng c√≥ d·ªØ li·ªáu (l·ªói ƒë√£ ƒë∆∞·ª£c b√°o ·ªü tr√™n).\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(\"\\n--- HO√ÄN T·∫§T ---\")\n",
    "print(f\"T·∫£i th√†nh c√¥ng: {successful_count} m√£\")\n",
    "print(f\"T·∫£i th·∫•t b·∫°i:     {failed_count} m√£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42205764",
   "metadata": {},
   "source": [
    "## 2. Ch·ªâ s·ªë th·ªã tr∆∞·ªùng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203b9b6",
   "metadata": {},
   "source": [
    "2.1. T·∫£i d·ªØ li·ªáu l·ªãch s·ª≠ - Th∆∞ vi·ªán vnstock.\n",
    "\n",
    "2.2. Chu·∫©n h√≥a d·ªØ li·ªáu: ƒê∆∞a v·ªÅ chu·∫©n chung cho ph√¢n t√≠ch k·ªπ thu·∫≠t - ƒê·ªïi t√™n c·ªôt; X·ª≠ l√Ω th·ªùi gian: Chuy·ªÉn c·ªôt th·ªùi gian v·ªÅ d·∫°ng datetime v√† ƒë·∫∑t l√†m Index (m·ª•c l·ª•c) cho DataFrame; S·∫Øp x·∫øp: ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp theo th·ª© t·ª± th·ªùi gian tƒÉng d·∫ßn.\n",
    "\n",
    "2.3. L∆∞u tr·ªØ: L∆∞u v√†o file data_ohlcv/VNINDEX_OHLCV.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3962d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from vnstock import Vnstock\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os # Th√™m th∆∞ vi·ªán os\n",
    "\n",
    "# --- 1. C·∫§U H√åNH ---\n",
    "# L∆∞u v√†o c√πng th∆∞ m·ª•c v·ªõi d·ªØ li·ªáu CafeF\n",
    "DATA_FOLDER = 'data_ohlcv' \n",
    "START_DATE_DEFAULT = '2019-01-01' # THAY ƒê·ªîI: L·∫•y t·ª´ 2019\n",
    "END_DATE_PROJECT = '2025-01-01'   # ƒê·ªÉ bao g·ªìm to√†n b·ªô nƒÉm 2024\n",
    "\n",
    "def get_vnindex_data(start_date=START_DATE_DEFAULT, end_date=None, data_folder=DATA_FOLDER):\n",
    "    \"\"\"\n",
    "    L·∫•y d·ªØ li·ªáu l·ªãch s·ª≠ VNINDEX (OHLCV) ƒë√£ chu·∫©n h√≥a v√† l∆∞u th√†nh CSV\n",
    "    \n",
    "    Parameters:\n",
    "    - start_date: ng√†y b·∫Øt ƒë·∫ßu (m·∫∑c ƒë·ªãnh: '2019-01-01')\n",
    "    - end_date: ng√†y k·∫øt th√∫c (m·∫∑c ƒë·ªãnh: ng√†y hi·ªán t·∫°i)\n",
    "    - data_folder: th∆∞ m·ª•c l∆∞u file\n",
    "    \"\"\"\n",
    "    \n",
    "    # N·∫øu kh√¥ng c√≥ end_date, l·∫•y ng√†y hi·ªán t·∫°i\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"ƒêang t·∫£i d·ªØ li·ªáu VNINDEX t·ª´ {start_date} ƒë·∫øn {end_date}...\")\n",
    "    \n",
    "    # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    df = None\n",
    "    sources_to_try = ['VCI', 'TCBS'] # C√°c ngu·ªìn ƒë·ªÉ th·ª≠\n",
    "    \n",
    "    # T·ª± ƒë·ªông th·ª≠ c√°c ngu·ªìn\n",
    "    for source in sources_to_try:\n",
    "        try:\n",
    "            print(f\"    -> ƒêang th·ª≠ ngu·ªìn: {source}\")\n",
    "            stock = Vnstock().stock(symbol='VNINDEX', source=source)\n",
    "            df = stock.quote.history(start=start_date, end=end_date)\n",
    "            \n",
    "            if df is not None and not df.empty:\n",
    "                print(f\"    -> T·∫£i th√†nh c√¥ng t·ª´ {source}.\")\n",
    "                break # T·∫£i th√†nh c√¥ng, tho√°t v√≤ng l·∫∑p\n",
    "            else:\n",
    "                print(f\"    -> Ngu·ªìn {source} tr·∫£ v·ªÅ r·ªóng.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó L·ªói v·ªõi ngu·ªìn {source}: {e}\")\n",
    "\n",
    "    # Ki·ªÉm tra l·∫ßn cu·ªëi sau khi th·ª≠ h·∫øt c√°c ngu·ªìn\n",
    "    if df is None or df.empty:\n",
    "        print(\"‚úó L·ªói: Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu VNINDEX t·ª´ m·ªçi ngu·ªìn.\")\n",
    "        return None\n",
    "        \n",
    "    # --- THAY ƒê·ªîI 2: X·ª¨ L√ù V√Ä CHU·∫®N H√ìA D·ªÆ LI·ªÜU ---\n",
    "    try:\n",
    "        # 1. ƒê·∫∑t Index (n·∫øu ch∆∞a ph·∫£i l√† index)\n",
    "        if 'time' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['time'])\n",
    "            df.set_index('date', inplace=True)\n",
    "        elif 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df.set_index('date', inplace=True)\n",
    "        \n",
    "        # 2. ƒê·ªïi t√™n c·ªôt sang ch·ªØ th∆∞·ªùng (R·∫§T QUAN TR·ªåNG cho pandas-ta)\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        # 3. Ch·ªçn c√°c c·ªôt OHLCV chu·∫©n\n",
    "        # VNINDEX kh√¥ng c√≥ 'adjusted_close', ch√∫ng ta d√πng 'close'\n",
    "        required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        \n",
    "        if all(col in df.columns for col in required_cols):\n",
    "            df = df[required_cols]\n",
    "        else:\n",
    "            print(\"‚úó L·ªói: D·ªØ li·ªáu tr·∫£ v·ªÅ thi·∫øu c√°c c·ªôt OHLCV.\")\n",
    "            print(f\"    C√°c c·ªôt c√≥: {list(df.columns)}\")\n",
    "            return None\n",
    "            \n",
    "        # 4. S·∫Øp x·∫øp l·∫°i theo ng√†y\n",
    "        df.sort_index(ascending=True, inplace=True)\n",
    "            \n",
    "        # 5. L∆∞u file\n",
    "        filename = f'VNINDEX_OHLCV.csv' # ƒê·∫∑t t√™n file ƒë∆°n gi·∫£n\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "        df.to_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"‚úì ƒê√£ l∆∞u d·ªØ li·ªáu chu·∫©n h√≥a v√†o file: {file_path}\")\n",
    "        print(f\"‚úì T·ªïng s·ªë d√≤ng d·ªØ li·ªáu: {len(df)}\")\n",
    "        print(f\"\\nXem tr∆∞·ªõc d·ªØ li·ªáu (ƒë√£ chu·∫©n h√≥a OHLCV):\")\n",
    "        print(df.head())\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó L·ªói khi x·ª≠ l√Ω DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "# S·ª≠ d·ª•ng h√†m\n",
    "if __name__ == \"__main__\":\n",
    "    # L·∫•y d·ªØ li·ªáu t·ª´ 2019 ƒë·∫øn h·∫øt 2024 (theo m·ªëc 01/01/2025)\n",
    "    vnindex_data = get_vnindex_data(start_date=START_DATE_DEFAULT, end_date=END_DATE_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1b415",
   "metadata": {},
   "source": [
    "## 3. T·ªïng h·ª£p v√† chu·∫©n h√≥a d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7a464",
   "metadata": {},
   "source": [
    "3.1. ƒê·ªìng b·ªô h√≥a d·ªØ li·ªáu: M√£ ngu·ªìn ƒë·ªçc file ch·ªâ s·ªë th·ªã tr∆∞·ªùng VNINDEX_OHLCV.csv v√† t·∫•t c·∫£ c√°c file c·ªï phi·∫øu trong th∆∞ m·ª•c data_ohlcv, sau ƒë√≥ chu·∫©n h√≥a t√™n c·ªôt, g√°n th√™m c·ªôt ticker cho t·ª´ng b·∫£ng d·ªØ li·ªáu v√† ƒë·∫£m b·∫£o m·ªói file ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt chu·∫©n. T·∫•t c·∫£ d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω theo c√πng m·ªôt ƒë·ªãnh d·∫°ng Long Format v√† ƒë∆∞·ª£c ƒë∆∞a v√†o m·ªôt danh s√°ch ƒë·ªÉ chu·∫©n b·ªã g·ªôp.\n",
    "\n",
    "3.2. S√†ng l·ªçc c·ªï phi·∫øu: Trong qu√° tr√¨nh duy·ªát danh s√°ch c·ªï phi·∫øu, m√£ ngu·ªìn t·ª± ƒë·ªông b·ªè qua c√°c c·ªï phi·∫øu n·∫±m trong REMOVE_STOCKS, v√¨ ƒë√¢y l√† c√°c m√£ ni√™m y·∫øt mu·ªôn ho·∫∑c thi·∫øu d·ªØ li·ªáu d√†i h·∫°n.\n",
    "\n",
    "3.3. L√†m s·∫°ch d·ªØ li·ªáu \n",
    "\n",
    "3.4. Xu·∫•t d·ªØ li·ªáu :L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng v√†o file all_data_merged.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478630b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 1. C·∫§U H√åNH ---\n",
    "DATA_FOLDER = 'data_ohlcv'\n",
    "OUTPUT_FILE = 'all_data_merged.csv' # File k·∫øt qu·∫£ d·∫°ng Long\n",
    "\n",
    "# Danh s√°ch m√£ c·∫ßn x·ª≠ l√Ω\n",
    "STOCK_LIST = [\n",
    "    'ACB', 'BID', 'CTG', 'EIB', 'HDB', 'LPB', 'MBB',\n",
    "    'MSB', 'NAB', 'OCB', 'SHB', 'SSB', 'STB', 'TCB',\n",
    "    'TPB', 'VCB', 'VIB', 'VPB'\n",
    "]\n",
    "\n",
    "# C√°c m√£ mu·ªën lo·∫°i b·ªè kh·ªèi k·∫øt qu·∫£ cu·ªëi c√πng\n",
    "REMOVE_STOCKS = ['MSB', 'NAB', 'OCB', 'SSB', 'SHB']\n",
    "\n",
    "# T√™n file v√† c·∫•u h√¨nh cho VNINDEX\n",
    "INDEX_FILENAME_PREFIX = 'VNINDEX_OHLCV'\n",
    "INDEX_TICKER_NAME = 'VNINDEX' # ƒê·∫∑t t√™n ticker cho ch·ªâ s·ªë n√†y trong c·ªôt 'ticker'\n",
    "\n",
    "# Danh s√°ch c√°c c·ªôt chu·∫©n mu·ªën gi·ªØ l·∫°i\n",
    "REQUIRED_COLUMNS = ['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "print(f\"üöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH G·ªòP D·ªÆ LI·ªÜU (LONG FORMAT) T·ª™ '{DATA_FOLDER}'...\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# H√ÄM H·ªñ TR·ª¢ ƒê·ªåC V√Ä CHU·∫®N H√ìA\n",
    "# ==============================================================================\n",
    "def process_file(file_path, ticker_name, is_index=False):\n",
    "    try:\n",
    "        # 1. ƒê·ªçc file\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {file_path}. B·ªè qua.\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "        \n",
    "        # 2. Th√™m c·ªôt 'ticker' (Quan tr·ªçng nh·∫•t c·ªßa Long Format)\n",
    "        df['ticker'] = ticker_name\n",
    "        \n",
    "        # 3. Chu·∫©n h√≥a t√™n c·ªôt v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        # 4. X·ª≠ l√Ω c·ªôt 'adjusted_close' (VNINDEX th∆∞·ªùng kh√¥ng c√≥, C·ªï phi·∫øu th√¨ c√≥)\n",
    "        # N·∫øu b·∫°n mu·ªën gi·ªØ adjusted_close, th√™m v√†o danh s√°ch c·ªôt\n",
    "        cols_to_take = REQUIRED_COLUMNS.copy()\n",
    "        if 'adjusted_close' in df.columns:\n",
    "             cols_to_take.append('adjusted_close')\n",
    "        elif is_index:\n",
    "             # V·ªõi VNINDEX, adjusted_close ch√≠nh l√† close\n",
    "             df['adjusted_close'] = df['close']\n",
    "             cols_to_take.append('adjusted_close')\n",
    "\n",
    "        # 5. Ch·ªâ l·∫•y c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªìng nh·∫•t\n",
    "        # (S·ª≠ d·ª•ng intersection ƒë·ªÉ tr√°nh l·ªói n·∫øu thi·∫øu c·ªôt n√†o ƒë√≥ l·∫°)\n",
    "        available_cols = [c for c in cols_to_take if c in df.columns]\n",
    "        df = df[available_cols]\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå L·ªói khi x·ª≠ l√Ω {ticker_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. X·ª¨ L√ù FILE CH·ªà S·ªê (VNINDEX)\n",
    "# ==============================================================================\n",
    "index_file_path = os.path.join(DATA_FOLDER, f\"{INDEX_FILENAME_PREFIX}.csv\")\n",
    "print(f\"üìÑ ƒêang x·ª≠ l√Ω ch·ªâ s·ªë th·ªã tr∆∞·ªùng ({INDEX_TICKER_NAME})...\")\n",
    "\n",
    "index_df = process_file(index_file_path, INDEX_TICKER_NAME, is_index=True)\n",
    "if index_df is not None:\n",
    "    all_dataframes.append(index_df)\n",
    "    print(f\"   -> Th√™m th√†nh c√¥ng {len(index_df)} d√≤ng d·ªØ li·ªáu VNINDEX.\")\n",
    "else:\n",
    "    print(\"   ‚ùå L·ªói nghi√™m tr·ªçng: Kh√¥ng c√≥ d·ªØ li·ªáu VNINDEX.\")\n",
    "    sys.exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. X·ª¨ L√ù C√ÅC FILE C·ªî PHI·∫æU\n",
    "# ==============================================================================\n",
    "print(f\"\\nüìÑ ƒêang x·ª≠ l√Ω {len(STOCK_LIST)} m√£ c·ªï phi·∫øu...\")\n",
    "count = 0\n",
    "\n",
    "for stock in STOCK_LIST:\n",
    "    # B·ªè qua n·∫øu m√£ n·∫±m trong danh s√°ch lo·∫°i b·ªè (x·ª≠ l√Ω s·ªõm ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ)\n",
    "    if stock in REMOVE_STOCKS:\n",
    "        print(f\"   -> B·ªè qua {stock} (n·∫±m trong danh s√°ch lo·∫°i b·ªè).\")\n",
    "        continue\n",
    "        \n",
    "    file_path = os.path.join(DATA_FOLDER, f\"{stock}.csv\")\n",
    "    stock_df = process_file(file_path, stock)\n",
    "    \n",
    "    if stock_df is not None:\n",
    "        all_dataframes.append(stock_df)\n",
    "        count += 1\n",
    "\n",
    "print(f\"   -> ƒê√£ th√™m th√†nh c√¥ng {count} m√£ c·ªï phi·∫øu.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. G·ªòP V√Ä L∆ØU D·ªÆ LI·ªÜU\n",
    "# ==============================================================================\n",
    "if not all_dataframes:\n",
    "    print(\"\\n‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë·ªÉ g·ªôp.\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\nüîÑ ƒêang g·ªôp t·∫•t c·∫£ d·ªØ li·ªáu (Stacking)...\")\n",
    "\n",
    "# D√πng axis=0 ƒë·ªÉ x·∫øp ch·ªìng c√°c dataframe l√™n nhau (Long Format)\n",
    "final_df = pd.concat(all_dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i theo Ticker -> Date\n",
    "print(\"‚ú® ƒêang s·∫Øp x·∫øp d·ªØ li·ªáu...\")\n",
    "final_df = final_df.sort_values(by=['ticker', 'date'])\n",
    "\n",
    "# L∆∞u file\n",
    "output_path = os.path.join(DATA_FOLDER, OUTPUT_FILE)\n",
    "try:\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ HO√ÄN T·∫§T! ƒê√£ l∆∞u file Long Format ƒë·∫ßy ƒë·ªß c·ªôt t·∫°i: {output_path}\")\n",
    "    \n",
    "    print(\"\\n--- M·∫™U D·ªÆ LI·ªÜU (5 d√≤ng ƒë·∫ßu) ---\")\n",
    "    print(final_df.head())\n",
    "    \n",
    "    print(\"\\n--- TH·ªêNG K√ä ---\")\n",
    "    print(f\"T·ªïng s·ªë d√≤ng: {len(final_df)}\")\n",
    "    print(f\"C√°c c·ªôt gi·ªØ l·∫°i: {list(final_df.columns)}\")\n",
    "    print(f\"S·ªë l∆∞·ª£ng m√£: {final_df['ticker'].nunique()}\")\n",
    "    print(f\"Danh s√°ch m√£: {final_df['ticker'].unique()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå L·ªñI KHI L∆ØU FILE: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae914d",
   "metadata": {},
   "source": [
    "## 4. Ki·ªÉm tra d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62615e96",
   "metadata": {},
   "source": [
    "4.1. Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa file:\n",
    "- X√°c nh·∫≠n xem file data_final.csv c√≥ t·ªìn t·∫°i v√† c√≥ m·ªü ƒë∆∞·ª£c kh√¥ng.\n",
    "- Xem nhanh 5 d√≤ng ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o c·∫•u tr√∫c b·∫£ng ƒë√∫ng √Ω mu·ªën.\n",
    "- Ki·ªÉm tra ƒë·ªãnh d·∫°ng: ƒê·∫£m b·∫£o c·ªôt date ƒë√£ ƒë∆∞·ª£c chuy·ªÉn ƒë√∫ng v·ªÅ d·∫°ng th·ªùi gian (datetime)\n",
    "\n",
    "4.2. Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu: C√≥ √¥ n√†o b·ªã NaN (Not a Number) hay Null kh√¥ng. \n",
    "\n",
    "4.3. Ki·ªÉm tra logic lo·∫°i b·ªè: X√°c minh l·∫°i xem c√°c m√£ c·ªï phi·∫øu mu·ªën lo·∫°i b·ªè (MSB, NAB, OCB, SSB, SHB) c√≥ th·ª±c s·ª± ƒë√£ bi·∫øn m·∫•t kh·ªèi d·ªØ li·ªáu ch∆∞a. \n",
    "\n",
    "4.4.Ki·ªÉm tra tr√πng l·∫∑p: C√≥ c·∫∑p (date, ticker) n√†o b·ªã l·∫∑p l·∫°i 2 l·∫ßn kh√¥ng. \n",
    "\n",
    "4.5. Ki·ªÉm tra ph·∫°m vi th·ªùi gian: X√°c nh·∫≠n d·ªØ li·ªáu c√≥ b·∫Øt ƒë·∫ßu t·ª´ nƒÉm 2019 (ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ giai ƒëo·∫°n \"warm-up\" cho c√°c ch·ªâ b√°o k·ªπ thu·∫≠t nh∆∞ MA200 ho·∫°t ƒë·ªông) v√† k·∫øt th√∫c ƒë√∫ng h·∫°n (kh√¥ng v∆∞·ª£t qu√° 01/01/2025) hay kh√¥ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4576e1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ==============================================================================\n",
    "# PH·∫¶N 1: C·∫§U H√åNH CHUNG\n",
    "# ==============================================================================\n",
    "DATA_FOLDER = 'data_ohlcv'\n",
    "INPUT_FILE = 'all_data_merged.csv'  # File n√†y b√¢y gi·ªù ƒê√É L√Ä d·∫°ng Long Format (ch·ª©a OHLCV)\n",
    "OUTPUT_FILE = 'data_final.csv'      # File ƒë·∫ßu ra (ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† ki·ªÉm tra)\n",
    "\n",
    "input_path = os.path.join(DATA_FOLDER, INPUT_FILE)\n",
    "output_path = os.path.join(DATA_FOLDER, OUTPUT_FILE)\n",
    "\n",
    "# Danh s√°ch c√°c m√£ c·∫ßn lo·∫°i b·ªè (Check l·∫°i l·∫ßn cu·ªëi cho ch·∫Øc)\n",
    "removed_stocks_check = ['MSB', 'NAB', 'OCB', 'SSB', 'SHB']\n",
    "\n",
    "# ==============================================================================\n",
    "# PH·∫¶N 2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU (KH√îNG C·∫¶N MELT N·ªÆA)\n",
    "# ==============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"B∆Ø·ªöC 1: CHU·∫®N B·ªä V√Ä S√ÄNG L·ªåC D·ªÆ LI·ªÜU (LONG FORMAT)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # 1. ƒê·ªçc d·ªØ li·ªáu\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file ƒë·∫ßu v√†o: {input_path}\")\n",
    "        sys.exit()\n",
    "        \n",
    "    # ƒê·ªçc file, pandas s·∫Ω t·ª± nh·∫≠n di·ªán c√°c c·ªôt OHLCV\n",
    "    df = pd.read_csv(input_path, parse_dates=['date'])\n",
    "    \n",
    "    print(f\"‚úì ƒê√£ ƒë·ªçc file: {INPUT_FILE}\")\n",
    "    print(f\"  - S·ªë d√≤ng: {len(df)}\")\n",
    "    print(f\"  - C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}\")\n",
    "\n",
    "    # 2. Ki·ªÉm tra c·∫•u tr√∫c Long Format\n",
    "    if 'ticker' not in df.columns:\n",
    "        print(\"‚ùå L·ªñI: File ƒë·∫ßu v√†o kh√¥ng c√≥ c·ªôt 'ticker'. Vui l√≤ng ki·ªÉm tra l·∫°i b∆∞·ªõc g·ªôp d·ªØ li·ªáu.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # 3. L·ªçc b·ªè c√°c m√£ r√°c (L·ªõp b·∫£o v·ªá th·ª© 2)\n",
    "    # M·∫∑c d√π b∆∞·ªõc tr∆∞·ªõc ƒë√£ l·ªçc, nh∆∞ng l√†m l·∫°i ·ªü ƒë√¢y ƒë·ªÉ ƒë·∫£m b·∫£o data_final s·∫°ch tuy·ªát ƒë·ªëi\n",
    "    print(f\"  - ƒêang r√† so√°t lo·∫°i b·ªè c√°c m√£: {removed_stocks_check}\")\n",
    "    df = df[~df['ticker'].isin(removed_stocks_check)]\n",
    "\n",
    "    # 4. S·∫Øp x·∫øp l·∫°i (Theo M√£ -> Ng√†y)\n",
    "    print(\"‚è≥ ƒêang s·∫Øp x·∫øp d·ªØ li·ªáu...\")\n",
    "    df = df.sort_values(by=['ticker', 'date'])\n",
    "\n",
    "    # 5. L∆∞u file k·∫øt qu·∫£ (ƒê√¢y l√† file ch√≠nh th·ª©c ƒë·ªÉ d√πng v·ªÅ sau)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ TH√ÄNH C√îNG! ƒê√£ l∆∞u file d·ªØ li·ªáu chu·∫©n t·∫°i: {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªñI NGHI√äM TR·ªåNG KHI X·ª¨ L√ù: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# PH·∫¶N 3: KI·ªÇM TRA D·ªÆ LI·ªÜU (VALIDATION)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"B∆Ø·ªöC 2: KI·ªÇM TRA CH·∫§T L∆Ø·ª¢NG FILE: {OUTPUT_FILE}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. T·∫£i l·∫°i file v·ª´a t·∫°o ƒë·ªÉ ki·ªÉm tra kh√°ch quan\n",
    "try:\n",
    "    df_check = pd.read_csv(output_path)\n",
    "    print(f\"‚úì ƒê√£ t·∫£i l·∫°i file ƒë·ªÉ ki·ªÉm tra: {OUTPUT_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªñI KHI ƒê·ªåC FILE ƒê·∫¶U RA: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- 1. Xem tr∆∞·ªõc d·ªØ li·ªáu (5 d√≤ng ƒë·∫ßu) ---\")\n",
    "# In ra ƒë·ªÉ b·∫°n th·∫•y r√µ c√°c c·ªôt open, high, low, close, volume v·∫´n c√≤n nguy√™n\n",
    "print(df_check.head())\n",
    "\n",
    "print(\"\\n--- 2. Ki·ªÉm tra Ki·ªÉu d·ªØ li·ªáu ---\")\n",
    "try:\n",
    "    df_check['date'] = pd.to_datetime(df_check['date'])\n",
    "    print(\"‚úì C·ªôt 'date' chu·∫©n ƒë·ªãnh d·∫°ng datetime.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói ƒë·ªãnh d·∫°ng c·ªôt 'date': {e}\")\n",
    "\n",
    "print(\"\\n--- 3. Ki·ªÉm tra Gi√° tr·ªã thi·∫øu (NaN) ---\")\n",
    "missing_report = df_check.isnull().sum()\n",
    "if missing_report.sum() == 0:\n",
    "    print(\"‚úì HO√ÄN H·∫¢O: Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu (NaN).\")\n",
    "else:\n",
    "    print(\"‚ùå C·∫¢NH B√ÅO: C√≥ gi√° tr·ªã thi·∫øu:\")\n",
    "    print(missing_report[missing_report > 0])\n",
    "    # T·ª± ƒë·ªông s·ª≠a l·ªói: X√≥a d√≤ng thi·∫øu\n",
    "    print(\"  -> ƒêang t·ª± ƒë·ªông x√≥a c√°c d√≤ng NaN...\")\n",
    "    df_check = df_check.dropna()\n",
    "    df_check.to_csv(output_path, index=False)\n",
    "    print(\"  -> ƒê√£ x√≥a xong v√† c·∫≠p nh·∫≠t file.\")\n",
    "\n",
    "print(\"\\n--- 4. Ki·ªÉm tra C·ªôt 'ticker' ---\")\n",
    "tickers_list = df_check['ticker'].unique()\n",
    "print(f\"‚úì T·ªïng s·ªë m√£: {len(tickers_list)}\")\n",
    "# Ki·ªÉm tra m√£ b·ªã lo·∫°i\n",
    "found_removed = [t for t in tickers_list if t in removed_stocks_check]\n",
    "if not found_removed:\n",
    "    print(f\"‚úì T·ªët: C√°c m√£ r√°c ({', '.join(removed_stocks_check)}) kh√¥ng t·ªìn t·∫°i.\")\n",
    "else:\n",
    "    print(f\"‚ùå C·∫¢NH B√ÅO: V·∫´n c√≤n c√°c m√£ c·∫ßn lo·∫°i b·ªè: {found_removed}\")\n",
    "\n",
    "print(\"\\n--- 5. Ki·ªÉm tra Tr√πng l·∫∑p ---\")\n",
    "duplicates = df_check.duplicated(subset=['date', 'ticker']).sum()\n",
    "if duplicates == 0:\n",
    "    print(\"‚úì HO√ÄN H·∫¢O: Kh√¥ng c√≥ d·ªØ li·ªáu tr√πng l·∫∑p.\")\n",
    "else:\n",
    "    print(f\"‚ùå C·∫¢NH B√ÅO: C√≥ {duplicates} d√≤ng b·ªã tr√πng l·∫∑p.\")\n",
    "\n",
    "print(\"\\n--- 6. Ki·ªÉm tra Ph·∫°m vi th·ªùi gian ---\")\n",
    "if 'date' in df_check.columns:\n",
    "    min_date = df_check['date'].min().date()\n",
    "    max_date = df_check['date'].max().date()\n",
    "    print(f\"‚úì D·ªØ li·ªáu t·ª´: {min_date} ƒë·∫øn {max_date}\")\n",
    "    \n",
    "    if min_date.year > 2019:\n",
    "         print(\"‚ö†Ô∏è L∆∞u √Ω: D·ªØ li·ªáu b·∫Øt ƒë·∫ßu sau nƒÉm 2019.\")\n",
    "    else:\n",
    "         print(\"‚úì T·ªët: D·ªØ li·ªáu bao g·ªìm giai ƒëo·∫°n 2019.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HO√ÄN T·∫§T QU√Å TR√åNH.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc09075",
   "metadata": {},
   "source": [
    "## 5. T·ª∑ su·∫•t sinh l·ª£i Logarit theo ng√†y cho t·ª´ng m√£ c·ªï phi·∫øu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce176b68",
   "metadata": {},
   "source": [
    "5.1. Chu·∫©n b·ªã d·ªØ li·ªáu: T·∫£i file data_final.csv.\n",
    "S·∫Øp x·∫øp l·∫°i (Sorting): ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c x·∫øp theo t·ª´ng m√£ c·ªï phi·∫øu ri√™ng bi·ªát, v√† trong m·ªói m√£, th·ªùi gian ch·∫°y t·ª´ qu√° kh·ª© ƒë·∫øn hi·ªán t·∫°i.  \n",
    "\n",
    "5.2. T√≠nh to√°n c·ªët l√µi: M√£ s·ª≠ d·ª•ng c√¥ng th·ª©c Log Return = ln(Pt/(Pt-1)). C∆° ch·∫ø groupby('ticker') ƒë√≥ng vai tr√≤ \"c√°ch ly\". N√≥ ƒë·∫£m b·∫£o vi·ªác l·∫•y gi√° ng√†y h√¥m tr∆∞·ªõc (shift(1)) ch·ªâ di·ªÖn ra trong n·ªôi b·ªô t·ª´ng m√£ c·ªï phi·∫øu.\n",
    "\n",
    "5.3. C·∫≠p nh·∫≠t file: Sau khi t√≠nh to√°n xong, m·ªôt c·ªôt m·ªõi t√™n l√† log_return ƒë∆∞·ª£c th√™m v√†o DataFrame; l∆∞u ƒë√® k·∫øt qu·∫£ (bao g·ªìm c·ªôt m·ªõi) v√†o ch√≠nh file data_final.csv.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48a255",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. C·∫§U H√åNH\n",
    "# ==============================================================================\n",
    "DATA_FOLDER = 'data_ohlcv'\n",
    "FILENAME = \"data_final.csv\"\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß: data_ohlcv/data_final.csv\n",
    "input_file_path = os.path.join(DATA_FOLDER, FILENAME)\n",
    "\n",
    "print(f\"üöÄ B·∫ÆT ƒê·∫¶U T√çNH TO√ÅN T·ª∂ SU·∫§T SINH L·ª¢I (LOG RETURNS) TR√äN FILE: {input_file_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. T·∫¢I D·ªÆ LI·ªÜU\n",
    "# ==============================================================================\n",
    "try:\n",
    "    if not os.path.exists(input_file_path):\n",
    "        print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp '{input_file_path}'.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Quan tr·ªçng: parse_dates=['date'] ƒë·ªÉ pandas hi·ªÉu ƒë√∫ng c·ªôt ng√†y th√°ng\n",
    "    df = pd.read_csv(input_file_path, parse_dates=['date'])\n",
    "    print(f\"‚úì ƒê√£ t·∫£i th√†nh c√¥ng t·ªáp. T·ªïng s·ªë d√≤ng: {len(df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i t·ªáp: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. T√çNH TO√ÅN LOG RETURNS (D√ôNG ADJUSTED CLOSE)\n",
    "# ==============================================================================\n",
    "if df is not None:\n",
    "    \n",
    "    # Ki·ªÉm tra xem c·ªôt adjusted_close c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "    if 'adjusted_close' not in df.columns:\n",
    "        print(\"‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng t√¨m th·∫•y c·ªôt 'adjusted_close' trong d·ªØ li·ªáu.\")\n",
    "        print(\"   Vui l√≤ng ki·ªÉm tra l·∫°i b∆∞·ªõc g·ªôp d·ªØ li·ªáu (c·∫ßn ƒë·∫£m b·∫£o gi·ªØ l·∫°i c·ªôt n√†y).\")\n",
    "        sys.exit()\n",
    "\n",
    "    # A. S·∫Øp x·∫øp d·ªØ li·ªáu (R·∫•t quan tr·ªçng)\n",
    "    print(\"\\n‚è≥ ƒêang s·∫Øp x·∫øp l·∫°i d·ªØ li·ªáu (Ticker -> Date)...\")\n",
    "    df = df.sort_values(by=['ticker', 'date'])\n",
    "\n",
    "    # B. T√≠nh to√°n\n",
    "    print(\"‚è≥ ƒêang t√≠nh to√°n Log Returns d·ª±a tr√™n GI√Å ƒêI·ªÄU CH·ªàNH (Adjusted Close)...\")\n",
    "    \n",
    "    # S·ª≠ d·ª•ng groupby ƒë·ªÉ ƒë·∫£m b·∫£o vi·ªác l·∫•y gi√° h√¥m tr∆∞·ªõc (shift) ch·ªâ di·ªÖn ra trong n·ªôi b·ªô t·ª´ng m√£\n",
    "    # C√¥ng th·ª©c M·ªöI: ln(Adj_Close_t / Adj_Close_t-1)\n",
    "    df['prev_adj_close'] = df.groupby('ticker')['adjusted_close'].shift(1)\n",
    "    \n",
    "    # T√≠nh log return\n",
    "    df['log_return'] = np.log(df['adjusted_close'] / df['prev_adj_close'])\n",
    "\n",
    "    # X√≥a c·ªôt ph·ª• 'prev_adj_close' ƒë·ªÉ file g·ªçn g√†ng\n",
    "    df.drop(columns=['prev_adj_close'], inplace=True)\n",
    "\n",
    "    # C. Ki·ªÉm tra k·∫øt qu·∫£\n",
    "    print(\"\\n--- Xem tr∆∞·ªõc k·∫øt qu·∫£ (5 d√≤ng ƒë·∫ßu c√≥ d·ªØ li·ªáu) ---\")\n",
    "    # Hi·ªán c·∫£ close v√† adjusted_close ƒë·ªÉ so s√°nh\n",
    "    cols_to_show = ['date', 'ticker', 'close', 'adjusted_close', 'log_return']\n",
    "    \n",
    "    # L·∫•y 5 d√≤ng ng·∫´u nhi√™n KH√îNG ph·∫£i NaN ƒë·ªÉ ki·ªÉm tra\n",
    "    print(df[df['log_return'].notna()][cols_to_show].head())\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 4. L∆ØU K·∫æT QU·∫¢\n",
    "    # ==============================================================================\n",
    "    try:\n",
    "        # L∆∞u ƒë√® l√™n file c≈©\n",
    "        df.to_csv(input_file_path, index=False)\n",
    "        print(f\"\\n‚úÖ TH√ÄNH C√îNG! ƒê√£ c·∫≠p nh·∫≠t c·ªôt 'log_return' (chu·∫©n adjusted) v√†o file: {input_file_path}\")\n",
    "        \n",
    "        # Th·ªëng k√™ nhanh\n",
    "        nan_count = df['log_return'].isnull().sum()\n",
    "        ticker_count = df['ticker'].nunique()\n",
    "        print(f\"   - T·ªïng s·ªë m√£ c·ªï phi·∫øu: {ticker_count}\")\n",
    "        print(f\"   - S·ªë l∆∞·ª£ng gi√° tr·ªã NaN: {nan_count} (B·∫±ng s·ªë l∆∞·ª£ng m√£ l√† ƒë√∫ng, do d√≤ng ƒë·∫ßu ti√™n m·ªói m√£ l√† NaN)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi l∆∞u t·ªáp: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae7500",
   "metadata": {},
   "source": [
    "## 6. T·∫°o ƒë·∫∑c tr∆∞ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5885a0b",
   "metadata": {},
   "source": [
    "6.1. T√≠nh to√°n Ch·ªâ b√°o K·ªπ thu·∫≠t:S·ª≠ d·ª•ng th∆∞ vi·ªán ta ƒë·ªÉ t√≠nh h√†ng lo·∫°t ch·ªâ b√°o cho t·ª´ng m√£ c·ªï phi·∫øu ri√™ng bi·ªát.\n",
    "- Xu h∆∞·ªõng (Trend): SMA10, SMA50 (ƒë∆∞·ªùng trung b√¨nh ƒë·ªông), MACD (ph√¢n k·ª≥ h·ªôi t·ª•).\n",
    "- ƒê·ªông l∆∞·ª£ng (Momentum): RSI (s·ª©c m·∫°nh t∆∞∆°ng ƒë·ªëi), Stochastic Oscillator.\n",
    "- Bi·∫øn ƒë·ªông (Volatility): ATR (bi√™n ƒë·ªô dao ƒë·ªông trung b√¨nh), Bollinger Bands, ƒê·ªô bi·∫øn ƒë·ªông l·ªãch s·ª≠ (Historical Volatility).\n",
    "- D√≤ng ti·ªÅn (Volume): OBV (On-Balance Volume), CMF (Chaikin Money Flow)\n",
    "\n",
    "6.2. T√≠nh to√°n Ch·ªâ b√°o Th·ªã tr∆∞·ªùng : T√°ch ri√™ng d·ªØ li·ªáu VNINDEX ƒë·ªÉ t√≠nh c√°c ch·ªâ b√°o vƒ© m√¥. M√£ t√≠nh to√°n RSI, ATR, Volatility ri√™ng cho VNINDEX v√† ƒë·∫∑t ti·ªÅn t·ªë VNI_ (v√≠ d·ª•: VNI_RSI_14).\n",
    "\n",
    "6.3. H·ª£p nh·∫•t d·ªØ li·ªáu: D√πng pd.merge ƒë·ªÉ \"g√°n\" c√°c ch·ªâ s·ªë c·ªßa VNINDEX v√†o t·ª´ng d√≤ng d·ªØ li·ªáu c·ªßa t·ª´ng c·ªï phi·∫øu. V√≠ d·ª•: D√≤ng d·ªØ li·ªáu ng√†y 18/11/2025 c·ªßa m√£ VCB s·∫Ω c√≥ th√™m c√°c c·ªôt nh∆∞ VNI_RSI_14 (RSI c·ªßa th·ªã tr∆∞·ªùng ng√†y h√¥m ƒë√≥).\n",
    "\n",
    "6.4. Chu·∫©n b·ªã cho Machine Learning: lo·∫°i b·ªè d√≤ng d·ªØ li·ªáu ri√™ng c·ªßa VNINDEX kh·ªèi file cu·ªëi c√πng. File k·∫øt qu·∫£ ch·ªâ c√≤n l·∫°i c√°c m√£ c·ªï phi·∫øu, nh∆∞ng m·ªói m√£ ƒë√£ ƒë∆∞·ª£c th√™m c√°c c·ªôt d·ªØ li·ªáu th√¥ng tin v·ªÅ th·ªã tr∆∞·ªùng. K·∫øt qu·∫£: File data_final.csv l√∫c n√†y ƒë√£ tr·ªü th√†nh m·ªôt b·ªô d·ªØ li·ªáu hu·∫•n luy·ªán (Training Dataset) ho√†n ch·ªânh, gi√†u th√¥ng tin, s·∫µn s√†ng ƒë·ªÉ ƒë∆∞a v√†o c√°c m√¥ h√¨nh d·ª± b√°o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e669881",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# C√°c ch·ªâ b√°o kƒ© thu·∫≠t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta # Th∆∞ vi·ªán Technical Analysis (c√†i ƒë·∫∑t b·∫±ng: pip install ta)\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# T·∫Øt c√°c c·∫£nh b√°o t·ª´ th∆∞ vi·ªán 'ta' v√† 'pandas'\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- 1. C·∫§U H√åNH ---\n",
    "DATA_FOLDER = 'data_ohlcv'\n",
    "FILENAME = \"data_final.csv\"\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß\n",
    "FILE_PATH = os.path.join(DATA_FOLDER, FILENAME)\n",
    "\n",
    "# --- C√ÅC H√ÄM T√çNH TO√ÅN ---\n",
    "\n",
    "def calculate_stock_indicators(group):\n",
    "    \"\"\"\n",
    "    H√†m n√†y t√≠nh to√°n t·∫•t c·∫£ c√°c ch·ªâ b√°o cho m·ªôt nh√≥m (m·ªôt m√£ ticker).\n",
    "    \"\"\"\n",
    "    # ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp theo ng√†y\n",
    "    group = group.sort_values('date')\n",
    "    \n",
    "    # 1. Ch·ªâ b√°o Xu h∆∞·ªõng (Trend)\n",
    "    group['SMA_10'] = ta.trend.SMAIndicator(group['close'], window=10).sma_indicator()\n",
    "    group['SMA_50'] = ta.trend.SMAIndicator(group['close'], window=50).sma_indicator()\n",
    "    \n",
    "    macd = ta.trend.MACD(group['close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "    group['MACD'] = macd.macd()\n",
    "    group['MACD_signal'] = macd.macd_signal()\n",
    "    group['MACD_hist'] = macd.macd_diff()\n",
    "    \n",
    "    # 2. Ch·ªâ b√°o ƒê·ªông l∆∞·ª£ng (Momentum)\n",
    "    group['RSI_14'] = ta.momentum.RSIIndicator(group['close'], window=14).rsi()\n",
    "    group['Stoch_K_14'] = ta.momentum.StochasticOscillator(\n",
    "        group['high'], group['low'], group['close'], window=14\n",
    "    ).stoch()\n",
    "    \n",
    "    # 3. Ch·ªâ b√°o Bi·∫øn ƒë·ªông (Volatility)\n",
    "    group['ATR_14'] = ta.volatility.AverageTrueRange(\n",
    "        group['high'], group['low'], group['close'], window=14\n",
    "    ).average_true_range()\n",
    "    \n",
    "    # 'log_return' ƒë√£ c√≥ s·∫µn t·ª´ b∆∞·ªõc tr∆∞·ªõc, d√πng n√≥ ƒë·ªÉ t√≠nh bi·∫øn ƒë·ªông l·ªãch s·ª≠\n",
    "    if 'log_return' in group.columns:\n",
    "        group['Historical_Vol_20'] = group['log_return'].rolling(window=20).std()\n",
    "    else:\n",
    "        # Ph√≤ng tr∆∞·ªùng h·ª£p ch∆∞a t√≠nh log_return, t√≠nh t·∫°m th·ªùi\n",
    "        group['log_return_temp'] = np.log(group['close'] / group['close'].shift(1))\n",
    "        group['Historical_Vol_20'] = group['log_return_temp'].rolling(window=20).std()\n",
    "        group.drop(columns=['log_return_temp'], inplace=True)\n",
    "    \n",
    "    bb = ta.volatility.BollingerBands(group['close'], window=20, window_dev=2)\n",
    "    group['BB_Middle'] = bb.bollinger_mavg()\n",
    "    group['BB_Upper'] = bb.bollinger_hband()\n",
    "    group['BB_Lower'] = bb.bollinger_lband()\n",
    "    # T√≠nh BB_Width chu·∫©n h√≥a\n",
    "    group['BB_Width'] = (group['BB_Upper'] - group['BB_Lower']) / group['BB_Middle']\n",
    "    \n",
    "    # 4. Ch·ªâ b√°o Kh·ªëi l∆∞·ª£ng (Volume)\n",
    "    group['OBV'] = ta.volume.OnBalanceVolumeIndicator(group['close'], group['volume']).on_balance_volume()\n",
    "    group['CMF_20'] = ta.volume.ChaikinMoneyFlowIndicator(\n",
    "        group['high'], group['low'], group['close'], group['volume'], window=20\n",
    "    ).chaikin_money_flow()\n",
    "    \n",
    "    return group\n",
    "\n",
    "def calculate_market_indicators(market_df):\n",
    "    \"\"\"\n",
    "    H√†m n√†y t√≠nh to√°n c√°c ch·ªâ b√°o cho VNINDEX v√† th√™m ti·ªÅn t·ªë VNI_.\n",
    "    \"\"\"\n",
    "    market_df = market_df.sort_values('date')\n",
    "    \n",
    "    # Ki·ªÉm tra xem log_return c√≥ s·∫µn kh√¥ng\n",
    "    if 'log_return' in market_df.columns:\n",
    "        market_df['VNI_log_return'] = market_df['log_return']\n",
    "        market_df['VNI_Hist_Vol_20'] = market_df['log_return'].rolling(window=20).std()\n",
    "    else:\n",
    "        # T√≠nh l·∫°i n·∫øu thi·∫øu\n",
    "        market_df['VNI_log_return'] = np.log(market_df['close'] / market_df['close'].shift(1))\n",
    "        market_df['VNI_Hist_Vol_20'] = market_df['VNI_log_return'].rolling(window=20).std()\n",
    "\n",
    "    market_df['VNI_ATR_14'] = ta.volatility.AverageTrueRange(\n",
    "        market_df['high'], market_df['low'], market_df['close'], window=14\n",
    "    ).average_true_range()\n",
    "    \n",
    "    market_df['VNI_RSI_14'] = ta.momentum.RSIIndicator(\n",
    "        market_df['close'], window=14\n",
    "    ).rsi()\n",
    "    \n",
    "    # Ch·ªçn c√°c c·ªôt ƒë·ªÉ h·ª£p nh·∫•t\n",
    "    market_features = [\n",
    "        'date', \n",
    "        'VNI_log_return', \n",
    "        'VNI_Hist_Vol_20', \n",
    "        'VNI_ATR_14', \n",
    "        'VNI_RSI_14'\n",
    "    ]\n",
    "    return market_df[market_features].copy()\n",
    "\n",
    "# --- CH∆Ø∆†NG TR√åNH CH√çNH ---\n",
    "\n",
    "print(f\"üöÄ B·∫ÆT ƒê·∫¶U T√çNH TO√ÅN CH·ªà B√ÅO K·ª∏ THU·∫¨T TR√äN FILE: {FILE_PATH}\")\n",
    "\n",
    "# 1. T·∫£i d·ªØ li·ªáu\n",
    "try:\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp '{FILE_PATH}'.\")\n",
    "        sys.exit()\n",
    "        \n",
    "    df = pd.read_csv(FILE_PATH, parse_dates=['date'])\n",
    "    print(f\"‚úì ƒê√£ t·∫£i d·ªØ li·ªáu th√†nh c√¥ng. T·ªïng s·ªë d√≤ng: {len(df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi ƒë·ªçc file: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# S·∫Øp x·∫øp d·ªØ li·ªáu\n",
    "df = df.sort_values(by=['ticker', 'date'])\n",
    "\n",
    "# --- B∆∞·ªõc 2: T√°ch v√† T√≠nh to√°n (Th·ªã tr∆∞·ªùng - VNINDEX) ---\n",
    "print(\"\\n--- 1. ƒêang t√≠nh to√°n ch·ªâ b√°o cho th·ªã tr∆∞·ªùng (VNINDEX)... ---\")\n",
    "\n",
    "# Ki·ªÉm tra xem VNINDEX c√≥ trong d·ªØ li·ªáu kh√¥ng\n",
    "if 'VNINDEX' not in df['ticker'].unique():\n",
    "    print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y m√£ 'VNINDEX' trong c·ªôt 'ticker'.\")\n",
    "    print(\"   Vui l√≤ng ki·ªÉm tra l·∫°i b∆∞·ªõc g·ªôp d·ªØ li·ªáu (merge_long_format).\")\n",
    "    sys.exit()\n",
    "\n",
    "df_market = df[df['ticker'] == 'VNINDEX'].copy()\n",
    "df_market_features = calculate_market_indicators(df_market)\n",
    "print(\"‚úì ƒê√£ t√≠nh xong ch·ªâ b√°o th·ªã tr∆∞·ªùng.\")\n",
    "\n",
    "# --- B∆∞·ªõc 3: T√°ch v√† T√≠nh to√°n (C·ªï phi·∫øu) ---\n",
    "print(\"\\n--- 2. ƒêang t√≠nh to√°n ch·ªâ b√°o cho c√°c c·ªï phi·∫øu ri√™ng l·∫ª... ---\")\n",
    "df_stocks = df[df['ticker'] != 'VNINDEX'].copy()\n",
    "\n",
    "unique_tickers = df_stocks['ticker'].nunique()\n",
    "print(f\"   ƒêang √°p d·ª•ng t√≠nh to√°n cho {unique_tickers} m√£ c·ªï phi·∫øu...\")\n",
    "\n",
    "# S·ª≠ d·ª•ng groupby.apply() ƒë·ªÉ t√≠nh to√°n ch·ªâ b√°o cho t·ª´ng m√£ c·ªï phi·∫øu\n",
    "# include_groups=False ƒë·ªÉ tr√°nh warning trong pandas b·∫£n m·ªõi (n·∫øu c√≥)\n",
    "try:\n",
    "    df_stocks_features = df_stocks.groupby('ticker', group_keys=False).apply(calculate_stock_indicators)\n",
    "except TypeError:\n",
    "    # Fallback cho phi√™n b·∫£n pandas c≈© h∆°n\n",
    "    df_stocks_features = df_stocks.groupby('ticker').apply(calculate_stock_indicators)\n",
    "    df_stocks_features = df_stocks_features.reset_index(drop=True)\n",
    "\n",
    "print(\"‚úì ƒê√£ t√≠nh xong ch·ªâ b√°o cho t·∫•t c·∫£ c·ªï phi·∫øu.\")\n",
    "\n",
    "# --- B∆∞·ªõc 4: H·ª£p nh·∫•t (Merge) ---\n",
    "print(\"\\n--- 3. ƒêang h·ª£p nh·∫•t d·ªØ li·ªáu th·ªã tr∆∞·ªùng v√†o d·ªØ li·ªáu c·ªï phi·∫øu... ---\")\n",
    "\n",
    "# L∆∞u √Ω: df_stocks_features b√¢y gi·ªù ch·ª©a c√°c ch·ªâ b√°o c·ªßa c·ªï phi·∫øu\n",
    "# Ch√∫ng ta merge df_market_features (ch·ªâ ch·ª©a date v√† ch·ªâ s·ªë VNI) v√†o ƒë√≥\n",
    "df_final = pd.merge(\n",
    "    df_stocks_features, \n",
    "    df_market_features, \n",
    "    on='date',            # Kh√≥a chung ƒë·ªÉ h·ª£p nh·∫•t\n",
    "    how='left'            # Gi·ªØ t·∫•t c·∫£ c√°c h√†ng c·ªï phi·∫øu\n",
    ")\n",
    "print(\"‚úì ƒê√£ h·ª£p nh·∫•t xong.\")\n",
    "\n",
    "# --- B∆∞·ªõc 5: L∆∞u k·∫øt qu·∫£ ---\n",
    "print(f\"\\n--- 4. ƒêang l∆∞u b·ªô d·ªØ li·ªáu ho√†n ch·ªânh v√†o {FILE_PATH}... ---\")\n",
    "try:\n",
    "    # S·∫Øp x·∫øp l·∫°i l·∫ßn cu·ªëi cho ƒë·∫πp\n",
    "    df_final = df_final.sort_values(by=['ticker', 'date'])\n",
    "    \n",
    "    # L∆∞u ƒë√® file g·ªëc\n",
    "    df_final.to_csv(FILE_PATH, index=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ ================== TH√ÄNH C√îNG! ==================\")\n",
    "    print(f\"ƒê√£ l∆∞u d·ªØ li·ªáu ML-ready v√†o: {FILE_PATH}\")\n",
    "    print(f\"File n√†y hi·ªán CH·ªà ch·ª©a d·ªØ li·ªáu c·ªï phi·∫øu (VNINDEX ƒë√£ ƒë∆∞·ª£c t√°ch ra v√† g·ªôp v√†o c·ªôt VNI_...)\")\n",
    "    \n",
    "    print(\"\\n--- 5 d√≤ng d·ªØ li·ªáu cu·ªëi c√πng trong file m·ªõi ---\")\n",
    "    cols_preview = ['date', 'ticker', 'close', 'RSI_14', 'VNI_RSI_14'] \n",
    "    # Ch·ªâ hi·ªán m·ªôt v√†i c·ªôt quan tr·ªçng ƒë·ªÉ check\n",
    "    print(df_final[cols_preview].tail())\n",
    "    \n",
    "    print(\"\\n--- Th·ªëng k√™ ---\")\n",
    "    print(f\"S·ªë l∆∞·ª£ng c·ªôt: {len(df_final.columns)}\")\n",
    "    print(f\"C√°c c·ªôt m·ªõi v√≠ d·ª•: {list(df_final.columns)[-5:]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå L·ªñI NGHI√äM TR·ªåNG khi l∆∞u file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc335f",
   "metadata": {},
   "source": [
    "## 7. Ki·ªÉm tra d·ªØ li·ªáu l·∫ßn cu·ªëi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15499fa",
   "metadata": {},
   "source": [
    "7.1. T·ª± ƒë·ªông s·ª≠a l·ªói d·ªØ li·ªáu khuy·∫øt: X√≥a b·ªè d·ªØ li·ªáu nƒÉm 2019 (d·ªØ li·ªáu warm up ƒë∆∞·ª£c d√πng ƒë·ªÉ t√≠nh to√°n ch·ªâ b√°o cho nƒÉm 2020), ƒë·∫£m b·∫£o ƒë√£ x√≥a 4 m√£ thi·∫øu d·ªØ li·ªáu v√† m√£ th·ªã tr∆∞·ªùng (VNINDEX) ƒë√£ bi·∫øn m·∫•t. ƒê·∫£m b·∫£o th·ªùi gian c·ªßa d·ªØ li·ªáu t·ª´ 2020-2024\n",
    "\n",
    "7.2. C·∫≠p nh·∫≠t d·ªØ li·ªáu: Sau khi ƒë√£ l·ªçc b·ªè h·∫øt NaN v√† ki·ªÉm tra k·ªπ c√†ng, script th·ª±c hi·ªán l∆∞u ƒë√® l√™n file data_final.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee031c1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra v√† L√†m s·∫°ch cu·ªëi c√πng (Final Validation)\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- 1. C·∫§U H√åNH ---\n",
    "DATA_FOLDER = 'data_ohlcv'\n",
    "FILE_NAME = \"data_final.csv\"\n",
    "FILE_PATH = os.path.join(DATA_FOLDER, FILE_NAME)\n",
    "\n",
    "# C·∫•u h√¨nh ki·ªÉm tra\n",
    "removed_stocks_check = ['MSB', 'NAB', 'OCB', 'SSB', 'SHB', 'VNINDEX'] \n",
    "# L∆∞u √Ω: VNINDEX c≈©ng c·∫ßn ƒë∆∞·ª£c lo·∫°i b·ªè kh·ªèi c·ªôt ticker v√¨ n√≥ ƒë√£ ƒë∆∞·ª£c t√°ch th√†nh c·ªôt ri√™ng (VNI_...)\n",
    "\n",
    "expected_start_date = pd.to_datetime('2020-01-01') # Ng√†y b·∫Øt ƒë·∫ßu mong mu·ªën cho M√¥ h√¨nh\n",
    "expected_end_date = pd.to_datetime('2025-01-01')\n",
    "\n",
    "print(f\"--- B·∫ÆT ƒê·∫¶U KI·ªÇM TRA V√Ä S·ª¨A L·ªñI FILE: {FILE_PATH} ---\")\n",
    "\n",
    "# --- 2. T·∫¢I V√Ä KI·ªÇM TRA C∆† B·∫¢N ---\n",
    "try:\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file '{FILE_PATH}'.\")\n",
    "        sys.exit()\n",
    "\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "    print(f\"‚úì ƒê√£ t·∫£i th√†nh c√¥ng file. T·ªïng s·ªë d√≤ng ban ƒë·∫ßu: {len(df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªñI KHI ƒê·ªåC FILE: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- 1. Ki·ªÉm tra Ki·ªÉu d·ªØ li·ªáu (dtypes) ---\")\n",
    "try:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    print(\"‚úì C·ªôt 'date' ƒë√£ ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi sang datetime.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi chuy·ªÉn ƒë·ªïi 'date': {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. KI·ªÇM TRA V√Ä X·ª¨ L√ù C√ÅC GI√Å TR·ªä THI·∫æU (NaN/Null) ---\n",
    "print(\"\\n--- 2. X·ª≠ l√Ω gi√° tr·ªã thi·∫øu (NaN) do t·∫°o ch·ªâ b√°o ---\")\n",
    "missing_report = df.isnull().sum()\n",
    "missing_total = missing_report.sum()\n",
    "\n",
    "if missing_total > 0:\n",
    "    print(f\"‚ö†Ô∏è Ph√°t hi·ªán {missing_total} √¥ tr·ªëng (NaN).\")\n",
    "    print(\"   (ƒêi·ªÅu n√†y l√† B√åNH TH∆Ø·ªúNG do c√°c ch·ªâ b√°o nh∆∞ SMA50, RSI c·∫ßn d·ªØ li·ªáu qu√° kh·ª© ƒë·ªÉ t√≠nh)\")\n",
    "    \n",
    "    # H√ÄNH ƒê·ªòNG S·ª¨A L·ªñI: X√≥a h√†ng NaN\n",
    "    print(\"   -> ƒêang x√≥a c√°c h√†ng ch·ª©a NaN (c·∫Øt b·ªè giai ƒëo·∫°n warm-up)...\")\n",
    "    original_count = len(df)\n",
    "    df = df.dropna()\n",
    "    new_count = len(df)\n",
    "    print(f\"   ‚úì ƒê√£ x√≥a {original_count - new_count} d√≤ng.\")\n",
    "    print(f\"   ‚úì S·ªë d√≤ng c√≤n l·∫°i: {new_count}\")\n",
    "else:\n",
    "    print(\"‚úì D·ªØ li·ªáu ƒë√£ s·∫°ch, kh√¥ng c√≥ NaN.\")\n",
    "\n",
    "# --- 4. L·ªåC THEO TH·ªúI GIAN (QUAN TR·ªåNG) ---\n",
    "print(\"\\n--- 3. L·ªçc d·ªØ li·ªáu theo khung th·ªùi gian mong mu·ªën ---\")\n",
    "print(f\"   Y√™u c·∫ßu: T·ª´ {expected_start_date.date()} ƒë·∫øn {expected_end_date.date()}\")\n",
    "\n",
    "# L·ªçc b·ªè d·ªØ li·ªáu tr∆∞·ªõc 2020 (NƒÉm 2019 ch·ªâ d√πng ƒë·ªÉ t√≠nh ch·ªâ b√°o, gi·ªù kh√¥ng c·∫ßn n·ªØa)\n",
    "df = df[(df['date'] >= expected_start_date) & (df['date'] <= expected_end_date)]\n",
    "print(f\"   ‚úì ƒê√£ c·∫Øt d·ªØ li·ªáu. S·ªë d√≤ng sau khi l·ªçc ng√†y: {len(df)}\")\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"‚ùå L·ªñI: Kh√¥ng c√≤n d·ªØ li·ªáu n√†o sau khi l·ªçc ng√†y! Vui l√≤ng ki·ªÉm tra l·∫°i range ng√†y.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 5. KI·ªÇM TRA LOGIC ---\n",
    "print(\"\\n--- 4. Ki·ªÉm tra Logic d·ªØ li·ªáu ---\")\n",
    "\n",
    "# Ki·ªÉm tra m√£ r√°c\n",
    "tickers_list = df['ticker'].unique()\n",
    "found_removed = [t for t in tickers_list if t in removed_stocks_check]\n",
    "\n",
    "if not found_removed:\n",
    "    print(f\"‚úì T·ªët: C√°c m√£ r√°c ({', '.join(removed_stocks_check)}) ƒë√£ s·∫°ch b√≥ng.\")\n",
    "else:\n",
    "    print(f\"‚ùå C·∫¢NH B√ÅO: V·∫´n t√¨m th·∫•y m√£ c·∫ßn lo·∫°i b·ªè: {found_removed}\")\n",
    "    # T·ª± ƒë·ªông x√≥a lu√¥n n·∫øu c√≤n s√≥t\n",
    "    df = df[~df['ticker'].isin(found_removed)]\n",
    "    print(f\"   -> ƒê√£ t·ª± ƒë·ªông x√≥a c√°c m√£ n√†y.\")\n",
    "\n",
    "# Ki·ªÉm tra tr√πng l·∫∑p\n",
    "duplicates = df.duplicated(subset=['date', 'ticker']).sum()\n",
    "if duplicates == 0:\n",
    "    print(f\"‚úì T·ªët: Kh√¥ng c√≥ d·ªØ li·ªáu tr√πng l·∫∑p (Ticker + Date).\")\n",
    "else:\n",
    "    print(f\"‚ùå C·∫¢NH B√ÅO: C√≥ {duplicates} d√≤ng tr√πng l·∫∑p. ƒêang x·ª≠ l√Ω...\")\n",
    "    df = df.drop_duplicates(subset=['date', 'ticker'])\n",
    "\n",
    "# --- 6. L∆ØU K·∫æT QU·∫¢ ---\n",
    "print(\"\\n--- 5. L∆ØU FILE FINAL ---\")\n",
    "try:\n",
    "    df.to_csv(FILE_PATH, index=False)\n",
    "    print(f\"‚úÖ HO√ÄN T·∫§T! ƒê√£ ghi ƒë√® d·ªØ li·ªáu s·∫°ch v√†o: {FILE_PATH}\")\n",
    "    \n",
    "    print(\"\\n--- TH·ªêNG K√ä CU·ªêI C√ôNG ---\")\n",
    "    print(df.head())\n",
    "    print(\"--------------------------\")\n",
    "    print(df.info())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªñI KHI L∆ØU FILE: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
